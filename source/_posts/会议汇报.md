第1页ppt

```
Hello, my name is Haihong Zhang, I am gonna present our work Optimizing Exploration-Exploitation Trade-off in Continuous Action Spaces via Q-ensemble
```

样本效率指的是利用有限的迭代过程中产生的数据收获尽可能多的经验，培养出一个成熟的智能体。

先解释样本效率。

第2页ppt

本文关注的重点在于样本效率和探索，它们也是强化学习中很经典的问题。首先是样本效率，在处理现实世界的物理系统时，数据收集可能是一个很困难的过程。无模型方法已经是可以从原始状态输入学习复杂策略的通用工具，但是它们普遍是以效率为代价的。RL方法通常需要数百万个训练样本才能解决一个任务[8,14]，样本效率低下严重限制了RL方法的适用性。

When dealing with real-world physical systems, for which data collection can be an arduous process。尤其是在无模型领域，Model-free methods have shown promise as a general-purpose tool for learning complex policies from raw state inputs , but their generality comes at the cost of effificiency.他的效率往往是特别低的。需要特别大量的数据。RL methods generally require millions of training samples to solve a task [8,14],the poor sample efficiency seriously limits the applicability of RL methods.本文的第一个方法就是在采样过程中提升样本效率。

```
The focus of this paper is on sample efficiency and exploration, which are also very classical problems in reinforcement learning. Model-free methods are already general tools that can learn complex strategies from raw state inputs, but they generally come at the cost of efficiency. RL methods generally require millions of training samples to solve a task, the poor sample efficiency seriously limits the applicability of RL methods.The first method in this paper is to optimize the sampling process to improve the sample efficiency.
```

为了进一步提升样本效率，智能体应该有效率地探索未知的环境，然后收集一些有利于智能体达到最优策略的交互数据，以便促进智能体的学习。所以探索的能力是尤为关键的，避免我们的策略陷入局部最优。

```
To further improve the sample efficiency, the agent should explore the unknown environment efficiently and then collect some interaction data that are favorable for the agent to reach the optimal strategy in order to facilitate the learning of the agent. 

So the agent's ability to explore is especially critical to avoid getting our strategy into a local optimum.
```



第3页ppt

有些方法是通过使用过去的序列来提升样本效率。首先是off-policy方法，它是用行为策略收集数据，存放到回放缓冲区中，事后反复的利用这些经验去更新目标策略。

```
Some methods are used to improve sample efficiency by using past transitions. The first is the off-policy approach, which collects data using behavioral policies, stores them in a replay buffer, and repeatedly uses these experiences to update the target policy afterward.
```

```
Model-based methods often attain higher sample efficiency by using a high UTD ratio,  the UTD ratio is defined as the number of Q-function updates divided by the number of actual interactions with the environment. If the UTD is set to n, it means that the agent interacts with the environment once to update the parameters n times.
```



第4页ppt

我们将我们的方法和high utd方法应用到TD3算法上进行对比。high utd方法是不断地重复采样数据和更新参数，但这种方式非常耗时。我们做了一点调整，只重复更新，不进行多次采样。这使得计算过程效率更高。

```
We compare our method with the high utd method applied to the TD3 algorithm. 
The high utd method is constantly repeating the sampling data and updating the parameters, but this approach is very time consuming. We have made a slight adjustment to just repeat the update without sampling multiple times.This makes the computational process more efficient.
```

但是在无模型方法中重复更新会引起很高的不稳定性，它会将高方差数据引起的估计偏差放大。所以我们需要在更新之间对数据进行筛选。具体的做法是先进行第一轮的更新，然后挑选一个排除高方差数据的子集，进行后续的更新。最终，我们就以很小的时间成本提升了样本效率。

```
However, repeated updates in the model-free method cause high instability, which amplifies the estimation bias caused by high variance data. So we need to filter the data between updates. The specific approach is to perform the first round of updates, then calculating the variance of the data using the results of the ensemble, and select a subset that excludes high variance data for subsequent updates.
Eventually, we then improve the sample efficiency with a small time cost.
```

q-functions的更新重复多次，但是只采样一次，不重复采样数据。

样本效率--》off-policy--》high UTD ratio算法（详解）--》耗时问题引入--》单批数据多次更新--》引入很高的方差（找证明？？？）--》利用集成网络计算所有数据的方差，挑选其中排除了高方差数据的子集，第一次更新之后重复更新该子集。以减缓高方差数据对算法的影响。以很小的时间成本在一定范围内提升了样本效率。



第5页ppt

介绍两类主流的探索算法。摘抄综述里的内容。

接下来，我们介绍两类主流的探索算法。首先是未知领域探索，通常是引导智能体去探索不确定性的区域，以此来实现效探索的。然后是Intrinsic Motivation-oriented Exploration,因为成就感通常伴随着积极的效果，就像是奖励，因此以内在动机为导向的探索方法往往设计内在奖励，为代理人创造成就感。

```
Next, we introduce two mainstream types of exploration algorithms. The first one is Uncertainty-oriented Exploration, which is usually used to guide the agent to explore regions of uncertainty as a way to achieve efficient exploration.

Then there is Intrinsic Motivation-oriented Exploration, because a sense of accomplishment is usually accompanied by positive effects, like rewards, thus intrinsic motivation-oriented exploration methods often design intrinsic rewards to create the sense of accomplishment for agents.
```

我们选择面向不确定性的探索We choose uncertainty-oriented exploration，We define an upper-confidence bound  based on the mean and variance of Q-functions。我们定义一个置信上界，然后对它进行最大化。我们用ensemble的值描述不确定性，然后鼓励代理探索未知区域。这个公式是探索领域的经典方法，通常用在离散动作空间中，我们将其放置在策略网络的损失函数上做梯度上升，生成唯一的动作，最终把它应用到连续动作空间当中。

，并且参数量很少，很容易确定每个环境的最优参数范围。

第6页ppt

```
We choose uncertainty-oriented exploration, We define an upper-confifidence bound  based on the mean and variance of ensemble.and then maximize on it. We characterize the uncertainty by the value of ensemble and then encourage the agent to explore the unknown areas. This formulation is a classical method in the exploration domain, usually used in discrete action spaces, and we place it on the loss function of the policy network to do gradient ascent to generate unique actions, and eventually apply it to continuous action spaces.
```

本文的提升样本效率和探索的技巧都使用了集成网络，为了保证网络差异性，……

```
Finally, both of our methods use q-ensemble, where all value networks are updated in the same way, so to prevent them from degenerating into a single network, we extract their parameters to construct a regular term to be placed into the loss function, enlarging the gap between parameters to increase ensemble diversity.
```



第7页ppt

```
In the paper, we conducted ablation experiments on both methods, as well as combining the two methods to compare with several similar methods, and the results proved that our method is simple and effective.
```

第8页ppt

```
That' s all the introduction of our work，thank you for listening!
```



---

基本方法是有明显提升效果的，但是我对其中的很多概念可能也会存在理解有误的情况，希望……